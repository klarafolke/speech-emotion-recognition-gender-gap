{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6b9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hej julia\n",
    "# hej igen julia\n",
    "# hej klara"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db165991",
   "metadata": {},
   "source": [
    "# Import  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a064aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sklearn \n",
    "import os \n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile \n",
    "import IPython.display as ipd\n",
    "from scipy import signal\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35bfa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d7c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05', 'Actor_06', 'Actor_07', 'Actor_08', 'Actor_09', 'Actor_10', 'Actor_11', 'Actor_12', 'Actor_13', 'Actor_14', 'Actor_15', 'Actor_16', 'Actor_17', 'Actor_18', 'Actor_19', 'Actor_20', 'Actor_21', 'Actor_22', 'Actor_23', 'Actor_24']\n"
     ]
    }
   ],
   "source": [
    "# Import data \n",
    "ravdessPath = \"./ravdess_data/audio_speech_actors_01-24/\"\n",
    "ravdessFiles = os.listdir(ravdessPath)\n",
    "print(ravdessFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c39af5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>./ravdess_data/audio_speech_actors_01-24/Actor...</td>\n",
       "      <td>Actor_01</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path     actor gender  \\\n",
       "0   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "1   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "2   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "3   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "4   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "5   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "6   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "7   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "8   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "9   ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "10  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "11  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "12  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "13  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "14  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "15  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "16  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "17  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "18  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "19  ./ravdess_data/audio_speech_actors_01-24/Actor...  Actor_01   male   \n",
       "\n",
       "    emotion emotion_label  \n",
       "0         1       neutral  \n",
       "1         1       neutral  \n",
       "2         1       neutral  \n",
       "3         1       neutral  \n",
       "4         2          calm  \n",
       "5         2          calm  \n",
       "6         2          calm  \n",
       "7         2          calm  \n",
       "8         2          calm  \n",
       "9         2          calm  \n",
       "10        2          calm  \n",
       "11        2          calm  \n",
       "12        3         happy  \n",
       "13        3         happy  \n",
       "14        3         happy  \n",
       "15        3         happy  \n",
       "16        3         happy  \n",
       "17        3         happy  \n",
       "18        3         happy  \n",
       "19        3         happy  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data frame\n",
    "ravdess_dataframe = pd.DataFrame(columns=['path','actor', 'gender', 'emotion','emotion_label'])\n",
    "count = 0\n",
    "\n",
    "# Loop through every actor\n",
    "for actor in ravdessFiles:\n",
    "    actorAudioFiles = os.listdir(ravdessPath + actor)\n",
    "    # Loop trough every audio file for the actor\n",
    "    for audioFile in actorAudioFiles:\n",
    "        nm = audioFile.split('.')[0].split('-')\n",
    "        path = ravdessPath + actor + '/' + audioFile\n",
    "        src = int(nm[1])\n",
    "        actorNum = int(nm[-1])\n",
    "        emotion = int(nm[2])\n",
    "\n",
    "        # get actors gender\n",
    "        if int(actorNum)%2 == 0:\n",
    "            gender = \"female\"\n",
    "        else:\n",
    "            gender = \"male\"\n",
    "\n",
    "        # Get emotion in audio file\n",
    "        if emotion == 1:\n",
    "            lb = \"neutral\"\n",
    "        elif emotion == 2:\n",
    "            lb = \"calm\"\n",
    "        elif emotion == 3:\n",
    "            lb = \"happy\"\n",
    "        elif emotion == 4:\n",
    "            lb = \"sad\"\n",
    "        elif emotion == 5:\n",
    "            lb = \"angry\"\n",
    "        elif emotion == 6:\n",
    "            lb = \"fearful\"\n",
    "        elif emotion == 7:\n",
    "            lb = \"disgust\"\n",
    "        elif emotion == 8:\n",
    "            lb = \"surprised\"\n",
    "        else:\n",
    "            lb = \"none\"\n",
    "\n",
    "        ravdess_dataframe.loc[count] = [path,actor, gender, emotion,lb]\n",
    "        count += 1\n",
    "\n",
    "ravdess_dataframe.sort_values(by='path',inplace=True)\n",
    "ravdess_dataframe.index =  range(len(ravdess_dataframe.index))\n",
    "ravdess_dataframe.head(20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe8443f6",
   "metadata": {},
   "source": [
    "# Plot audio test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf445f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julia\\AppData\\Local\\Temp\\ipykernel_30064\\2145195898.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  samplingFreq, mySound = wavfile.read(myAudio)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (158558,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m timeArray \u001b[39m=\u001b[39m timeArray \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[39m#Plot the tone\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m plt\u001b[39m.\u001b[39;49mplot(timeArray, mySoundOneChannel, color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mG\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     50\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mTime (ms)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAmplitude\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2813\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2814\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (158558,) and (1,)"
     ]
    }
   ],
   "source": [
    "\"\"\"Plots\n",
    "Time in MS Vs Amplitude in DB of a input wav signal\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "\n",
    "myAudio = ravdess_dataframe.path[0]\n",
    "\n",
    "#Read file and get sampling freq [ usually 44100 Hz ]  and sound object\n",
    "samplingFreq, mySound = wavfile.read(myAudio)\n",
    "\n",
    "#Check if wave file is 16bit or 32 bit. 24bit is not supported\n",
    "mySoundDataType = mySound.dtype\n",
    "\n",
    "#We can convert our sound array to floating point values ranging from -1 to 1 as follows\n",
    "\n",
    "mySound = mySound / (2.**15)\n",
    "\n",
    "#Check sample points and sound channel for duel channel(5060, 2) or  (5060, ) for mono channel\n",
    "\n",
    "mySoundShape = mySound.shape\n",
    "samplePoints = float(mySound.shape[0])\n",
    "\n",
    "#Get duration of sound file\n",
    "signalDuration =  mySound.shape[0] / samplingFreq\n",
    "\n",
    "#If two channels, then select only one channel\n",
    "mySoundOneChannel = mySound[0]\n",
    "\n",
    "#Plotting the tone\n",
    "\n",
    "\n",
    "# We can represent sound by plotting the pressure values against time axis.\n",
    "#Create an array of sample point in one dimension\n",
    "timeArray = numpy.arange(0, samplePoints, 1)\n",
    "\n",
    "#\n",
    "timeArray = timeArray / samplingFreq\n",
    "\n",
    "#Scale to milliSeconds\n",
    "timeArray = timeArray * 1000\n",
    "\n",
    "#Plot the tone\n",
    "plt.plot(timeArray, mySoundOneChannel, color='G')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot frequency content\n",
    "#We can get frquency from amplitude and time using FFT , Fast Fourier Transform algorithm\n",
    "\n",
    "#Get length of mySound object array\n",
    "mySoundLength = len(mySound)\n",
    "\n",
    "#Take the Fourier transformation on given sample point \n",
    "#fftArray = fft(mySound)\n",
    "fftArray = fft(mySoundOneChannel)\n",
    "\n",
    "numUniquePoints = numpy.ceil((mySoundLength + 1) / 2.0)\n",
    "fftArray = fftArray[0:numUniquePoints]\n",
    "\n",
    "#FFT contains both magnitude and phase and given in complex numbers in real + imaginary parts (a + ib) format.\n",
    "#By taking absolute value , we get only real part\n",
    "\n",
    "fftArray = abs(fftArray)\n",
    "\n",
    "#Scale the fft array by length of sample points so that magnitude does not depend on\n",
    "#the length of the signal or on its sampling frequency\n",
    "\n",
    "fftArray = fftArray / float(mySoundLength)\n",
    "\n",
    "#FFT has both positive and negative information. Square to get positive only\n",
    "fftArray = fftArray **2\n",
    "\n",
    "#Multiply by two (research why?)\n",
    "#Odd NFFT excludes Nyquist point\n",
    "if mySoundLength % 2 > 0: #we've got odd number of points in fft\n",
    "    fftArray[1:len(fftArray)] = fftArray[1:len(fftArray)] * 2\n",
    "\n",
    "else: #We've got even number of points in fft\n",
    "    fftArray[1:len(fftArray) -1] = fftArray[1:len(fftArray) -1] * 2  \n",
    "\n",
    "freqArray = numpy.arange(0, numUniquePoints, 1.0) * (samplingFreq / mySoundLength);\n",
    "\n",
    "#Plot the frequency\n",
    "plt.plot(freqArray/1000, 10 * numpy.log10 (fftArray), color='B')\n",
    "plt.xlabel('Frequency (Khz)')\n",
    "plt.ylabel('Power (dB)')\n",
    "plt.show()\n",
    "\n",
    "#Get List of element in frequency array\n",
    "#print freqArray.dtype.type\n",
    "freqArrayLength = len(freqArray)\n",
    "print(\"freqArrayLength =\", freqArrayLength)\n",
    "numpy.savetxt(\"freqData.txt\", freqArray, fmt='%6.2f')\n",
    "\n",
    "#Print FFtarray information\n",
    "print(\"fftArray length =\", len(fftArray))\n",
    "numpy.savetxt(\"fftData.txt\", fftArray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e335283",
   "metadata": {},
   "source": [
    "# Plot audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cb665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ravdess_data/audio_speech_actors_01-24/Actor_01/03-01-01-01-01-01-01.wav\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'axes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m freqs, times, spectrogram \u001b[39m=\u001b[39m log_specgram(samples, sample_rate)\n\u001b[0;32m     28\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m14\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m ax1 \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49madd_subplot(\u001b[39m211\u001b[39;49m)\n\u001b[0;32m     30\u001b[0m ax1\u001b[39m.\u001b[39mset_title(\u001b[39m'\u001b[39m\u001b[39mWaveform of the audio\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m ax1\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mAmplitude\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\figure.py:739\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    734\u001b[0m     \u001b[39m# Axes itself allows for a 'figure' kwarg, but since we want to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[39m# bind the created Axes to self, it is not allowed here.\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     \u001b[39mraise\u001b[39;00m _api\u001b[39m.\u001b[39mkwarg_error(\u001b[39m\"\u001b[39m\u001b[39madd_subplot\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    738\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 739\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], mpl\u001b[39m.\u001b[39;49maxes\u001b[39m.\u001b[39m_base\u001b[39m.\u001b[39m_AxesBase)\n\u001b[0;32m    740\u001b[0m         \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_subplotspec()):\n\u001b[0;32m    741\u001b[0m     ax \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m    742\u001b[0m     key \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39m_projection_init\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\_api\\__init__.py:226\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m props:\n\u001b[0;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m props[name]\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance)\n\u001b[1;32m--> 226\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'axes'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:99\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m# only call close('all') if any to close\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m# close triggers gc.collect, which can be slow\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m close \u001b[39mand\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 99\u001b[0m     matplotlib\u001b[39m.\u001b[39;49mpyplot\u001b[39m.\u001b[39mclose(\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Julia\\Documents\\source\\DA150X\\speech-emotion-recognization-gender-gap\\virtualEnvironment\\lib\\site-packages\\matplotlib\\_api\\__init__.py:226\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m props:\n\u001b[0;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m props[name]\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance)\n\u001b[1;32m--> 226\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'pyplot'"
     ]
    }
   ],
   "source": [
    "# Load the audio with sampling rate(sr) = 44100 ( which is the standard sr for high quality audio)\n",
    "sampling_rate = 44100\n",
    "\n",
    "filename = ravdess_dataframe.path[0]\n",
    "print (filename)\n",
    "\n",
    "\n",
    "samples, sample_rate = librosa.load(filename, sr=sampling_rate)\n",
    "sample_rate, samples.shape\n",
    "\n",
    "ipd.Audio(samples,rate=sample_rate)\n",
    "\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "# Plotting Wave Form and Spectrogram\n",
    "freqs, times, spectrogram = log_specgram(samples, sample_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Waveform of the audio')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "librosa.display.waveplot(samples, sr=sample_rate)\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.imshow(spectrogram.T, aspect='auto', origin='lower', \n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "ax2.set_yticks(freqs[::32])\n",
    "start, end = ax2.get_xlim()\n",
    "ax2.xaxis.set_ticks(np.arange(0.0, end, 0.5))\n",
    "ax2.set_title('Spectrogram of audio')\n",
    "ax2.set_ylabel('Freqs in Hz')\n",
    "ax2.set_xlabel('Seconds')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc88454",
   "metadata": {},
   "source": [
    "# Trim Audio\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e7d4221",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c608b885",
   "metadata": {},
   "source": [
    "# Creating training and testing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d99857",
   "metadata": {},
   "source": [
    "# Classification yippi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c6c99",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
